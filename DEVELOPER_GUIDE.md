# Developer Guide: 3D Gaussian Splatting

This document provides a detailed explanation of the operation and implementation of the 3D Gaussian Splatting codebase. It is intended for developers who want to understand the inner workings of the project and potentially contribute to it.

## Table of Contents

- [Core Concepts](#core-concepts)
- [Training Pipeline](#training-pipeline)
- [Rendering Pipeline](#rendering-pipeline)
- [Key Data Structures](#key-data-structures)
- [Code Modules](#code-modules)

## Core Concepts

The 3D Gaussian Splatting technique is a novel approach to novel-view synthesis that aims to achieve real-time rendering speeds while maintaining high visual quality. It represents the scene as a collection of 3D Gaussians, which are then rendered onto the image plane using a technique called splatting.

### 3D Gaussian Representation

The core idea of the method is to represent the scene using a set of 3D Gaussians. Each Gaussian is defined by the following parameters:

-   **Position (xyz):** The center of the Gaussian in 3D space.
-   **Covariance Matrix (Σ):** A 3x3 matrix that defines the shape and orientation of the Gaussian. In this implementation, the covariance matrix is represented by a scale and a rotation quaternion, which is more efficient to optimize.
-   **Color:** The color of the Gaussian, represented by spherical harmonics (SH) coefficients. This allows for view-dependent color effects.
-   **Opacity (α):** The transparency of the Gaussian.

This representation is more efficient than traditional radiance fields (like NeRF), as it does not require a deep neural network to be queried for every point in space. Instead, the scene is explicitly represented by the set of Gaussians.

### Optimization

The parameters of the Gaussians are optimized to reconstruct the training images. The optimization process involves the following steps:

1.  **Initialization:** The Gaussians are initialized from a sparse point cloud, typically generated by a Structure-from-Motion (SfM) algorithm like COLMAP.
2.  **Rendering:** For each training view, the Gaussians are rendered onto the image plane using a differentiable rasterizer. This produces a rendered image and an alpha mask.
3.  **Loss Calculation:** The rendered image is compared to the ground truth training image using a loss function that combines L1 loss and SSIM.
4.  **Backpropagation:** The loss is backpropagated through the differentiable rasterizer to compute the gradients of the loss with respect to the Gaussian parameters.
5.  **Parameter Update:** The Gaussian parameters are updated using the Adam optimizer.

### Densification and Pruning

A key innovation of this method is the adaptive densification of the Gaussians during optimization. This allows the model to capture fine details in the scene. The densification process involves:

-   **Cloning:** Gaussians in areas with high reconstruction error are cloned to create new Gaussians.
-   **Splitting:** Large Gaussians in areas with high reconstruction error are split into smaller Gaussians.

The model also prunes Gaussians that are too small or transparent, which helps to keep the number of Gaussians manageable.

### Real-time Rendering

The use of a highly optimized, CUDA-based rasterizer for splatting the Gaussians onto the image plane allows for real-time rendering speeds. The rasterizer efficiently culls Gaussians that are outside the view frustum and sorts them by depth to ensure correct blending. This, combined with the explicit representation of the scene, is what enables the method to achieve real-time performance.

## Training Pipeline

The training pipeline is orchestrated by the `train.py` script. It takes a dataset of images with corresponding camera poses and optimizes a `GaussianModel` to reconstruct those images.

The training process can be broken down into the following steps:

1.  **Initialization:**
    *   The script starts by parsing the command-line arguments, which include parameters for the model, optimization, and pipeline.
    *   A `GaussianModel` is created.
    *   A `Scene` object is initialized, which loads the training and testing cameras and the initial point cloud from the dataset.
    *   The `GaussianModel` is initialized with the point cloud from the `Scene`.
    -   The Adam optimizer is set up with different learning rates for the different parameters of the Gaussians (position, color, opacity, scale, rotation).

2.  **Training Loop:**
    *   The script iterates for a specified number of iterations. In each iteration:
        *   A random camera is selected from the training set.
        *   The `render` function is called to render the scene from the selected camera's viewpoint. This function is the core of the rendering process and is explained in more detail in the "Rendering Pipeline" section.
        *   The loss is calculated between the rendered image and the ground truth image. The loss is a combination of L1 loss and SSIM.
        *   The loss is backpropagated to compute the gradients of the loss with respect to the Gaussian parameters.
        *   The optimizer takes a step to update the Gaussian parameters.

3.  **Densification and Pruning:**
    *   The `GaussianModel` is adaptively densified during training. This is a crucial part of the method that allows it to capture fine details.
    -   After a certain number of iterations (`densify_from_iter`), the model starts the densification process.
    -   In each densification step, the model identifies Gaussians that need to be densified based on their gradients.
    -   Gaussians with large gradients and small size are **cloned**.
    -   Gaussians with large gradients and large size are **split** into smaller Gaussians.
    -   The model also prunes Gaussians that are too small or have a very low opacity. This helps to keep the number of Gaussians under control and removes unnecessary floaters.
    -   The densification process stops after a certain number of iterations (`densify_until_iter`).

4.  **Learning Rate Scheduling:**
    *   The learning rate for the position of the Gaussians is annealed during training using an exponential learning rate scheduler. This helps to stabilize the training process.

5.  **Logging and Checkpointing:**
    *   The training progress, including the loss and other metrics, is logged to TensorBoard for visualization.
    *   The model is saved at regular intervals, creating checkpoints that can be used to resume training or for rendering.

The `train.py` script also includes a network GUI that allows for real-time visualization of the training process. This is a very useful tool for debugging and for getting a better understanding of how the model is learning.

## Rendering Pipeline

The rendering pipeline is responsible for generating 2D images from the 3D Gaussian scene representation. The main script for offline rendering is `render.py`, while the real-time viewers provide an interactive rendering experience. The core rendering logic is implemented in the `gaussian_renderer` module.

The rendering process consists of the following steps:

1.  **Loading the Model:**
    *   The `render.py` script loads a trained `GaussianModel` from a checkpoint.
    *   It also loads the camera views for which the images are to be rendered (either the training or testing set).

2.  **Rendering a Single View:**
    *   For each camera view, the `render` function in `gaussian_renderer/__init__.py` is called.
    *   This function takes the camera parameters and the `GaussianModel` as input.

3.  **Rasterization Setup:**
    *   Inside the `render` function, a `GaussianRasterizationSettings` object is created. This object holds all the parameters needed for the rasterizer, such as the image dimensions, camera intrinsics and extrinsics, and background color.
    *   A `GaussianRasterizer` object is then created with these settings.

4.  **Data Preparation:**
    *   The parameters of the Gaussians (position, color, opacity, scale, rotation) are retrieved from the `GaussianModel`.
    *   The 3D covariance matrix for each Gaussian is computed from its scale and rotation.
    *   The color of each Gaussian is computed from its spherical harmonics coefficients and the viewing direction.

5.  **CUDA-based Rasterization:**
    *   The prepared data is passed to the `GaussianRasterizer` object.
    *   The rasterizer is a highly optimized CUDA extension that performs the following operations:
        *   **Projection:** It projects the 3D Gaussians onto the 2D image plane.
        *   **Culling:** It culls Gaussians that are outside the view frustum.
        *   **Sorting:** It sorts the Gaussians by depth to ensure correct blending.
        *   **Splatting:** It "splats" each Gaussian onto the image plane, which involves calculating the color and opacity contribution of the Gaussian to each pixel it covers.
    *   The rasterizer outputs the final rendered image.

6.  **Output:**
    *   The `render.py` script saves the rendered images to the disk.
    *   The real-time viewers display the rendered images on the screen.

The efficiency of the rendering pipeline is the key to the real-time performance of the 3D Gaussian Splatting method. The custom CUDA rasterizer is highly optimized for this specific task, which allows it to render millions of Gaussians at interactive frame rates.

## Key Data Structures

The codebase uses two main data structures to represent the scene: `GaussianModel` and `Camera`.

### `GaussianModel`

The `GaussianModel` class (defined in `scene/gaussian_model.py`) is the core data structure of the project. It represents the set of 3D Gaussians that form the scene.

**Key Attributes:**

-   `_xyz`: A tensor of shape `(N, 3)` storing the 3D position of each Gaussian.
-   `_features_dc`: A tensor storing the DC component of the spherical harmonics (SH) coefficients for each Gaussian. This represents the base color.
-   `_features_rest`: A tensor storing the higher-order SH coefficients, which capture view-dependent color effects.
-   `_scaling`: A tensor of shape `(N, 3)` storing the scaling factor for each Gaussian along its three principal axes.
-   `_rotation`: A tensor of shape `(N, 4)` storing the rotation of each Gaussian as a quaternion.
-   `_opacity`: A tensor of shape `(N, 1)` storing the opacity of each Gaussian.

All these attributes are `nn.Parameter` objects, which means they are automatically registered as trainable parameters by PyTorch.

**Key Methods:**

-   `create_from_pcd()`: Initializes the `GaussianModel` from a point cloud.
-   `training_setup()`: Sets up the optimizer and learning rate schedulers.
-   `densify_and_prune()`: Implements the logic for adaptive densification and pruning of the Gaussians.
-   `get_...()`: A set of property methods (e.g., `get_xyz`, `get_scaling`) that return the activated parameters (e.g., `exp(_scaling)`).
-   `save_ply()` and `load_ply()`: Methods for saving and loading the model to and from `.ply` files.

### `Camera`

The `Camera` class (defined in `scene/cameras.py`) represents a single camera in the scene.

**Key Attributes:**

-   `R`: The rotation matrix of the camera (world to camera).
-   `T`: The translation vector of the camera (world to camera).
-   `FoVx`, `FoVy`: The field of view of the camera in the x and y directions.
-   `original_image`: The ground truth image captured by the camera.
-   `world_view_transform`: The world-to-view transformation matrix.
-   `projection_matrix`: The projection matrix.
-   `full_proj_transform`: The full projection transformation matrix (world to screen).
-   `camera_center`: The position of the camera in world coordinates.

The `Camera` class provides all the necessary information to render the scene from a specific viewpoint.

## Code Modules

The codebase is organized into several modules, each with a specific responsibility.

### `gaussian_renderer`

This module contains the core rendering logic.

-   `__init__.py`: Defines the `render` function, which orchestrates the rendering process. It sets up the rasterizer and calls it to render the image.
-   The `diff_gaussian_rasterization` submodule (a CUDA extension) implements the differentiable Gaussian rasterizer.

### `scene`

This module is responsible for representing the 3D scene.

-   `gaussian_model.py`: Defines the `GaussianModel` class, which stores and manages the 3D Gaussians.
-   `cameras.py`: Defines the `Camera` class, which represents a camera in the scene.
-   `__init__.py`: Defines the `Scene` class, which manages the entire scene, including the cameras, the Gaussian model, and the training data.

### `utils`

This module contains various utility functions.

-   `general_utils.py`: General utility functions, such as `safe_state` for initializing the random number generators.
-   `graphics_utils.py`: Utility functions related to graphics, such as functions for creating projection matrices and transformation matrices.
-   `loss_utils.py`: Defines the loss functions used for training (L1 and SSIM).
-   `sh_utils.py`: Utility functions for working with spherical harmonics.
-   `system_utils.py`: Utility functions for system-related tasks, such as creating directories.

### `arguments`

This module defines the command-line arguments for the different scripts.

-   `__init__.py`: Defines the `ModelParams`, `OptimizationParams`, and `PipelineParams` classes, which are used to parse the command-line arguments.

### Root Directory Scripts

-   `train.py`: The main script for training the 3D Gaussian Splatting model.
-   `render.py`: The script for rendering images from a trained model.
-   `metrics.py`: The script for computing evaluation metrics.
-   `full_eval.py`: A script for running the full evaluation pipeline.
-   `convert.py`: A script for converting custom images into the required COLMAP format.

### `SIBR_viewers`

This directory contains the source code for the interactive viewers, which are based on the SIBR (System for Image-Based Rendering) framework. The viewers allow for real-time visualization of the training process and the trained models.
